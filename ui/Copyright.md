# Copyright - 3rd party resources used in this project


## Software

- Slider implementation for the main study interface based on [Responsive Slider implementation](https://codepen.io/thebabydino/pen/RwjWrKz) (created by [Ana Tudor](https://ko-fi.com/anatudor))
- Browser / Device Check implementation based on [Detect Mobile Browser tool](http://detectmobilebrowsers.com/) (created by [Chad Smith](https://twitter.com/chadsmith))
- Alerts created with [SweetAlert2](https://sweetalert2.github.io/) (created by [SweetAlert2](https://github.com/sweetalert2))

> [!NOTE]
> Check out our requirements.txt file to find all relevant Python packages!

## Images

### Colorblindness Check
- We use the [Ishihara Blind Test Cards](https://www.kaggle.com/datasets/dupeljan/ishihara-blind-test-cards) hosted on Kaggle © [dupeljan](https://www.kaggle.com/dupeljan)

### Focal Study Introduction
- Base image for first two pairs © [Leibniz University Hannover](https://www.uni-hannover.de/en/)
- Last image pair originates from the official [ColorFool GitHub repository](https://github.com/smartcameras/ColorFool/tree/master)

### Comprehension Check
- Clean Comprehension Check images are samples from [ImageNetV2 dataset](https://github.com/modestyachts/ImageNetV2)
- Modifications created manually with photofilters.com © [Zygomatic](https://www.zygomatic.nl/)

### Main Study
* Images classified as *unmodified* or *clean* are images from the [2012 ImageNet Validation Set](https://www.image-net.org/) © 2020 Stanford Vision Lab, Stanford University, Princeton University
    * *Modified* images are adversarially perturbed versions of these images 
* Attention check images are [ImageNet samples from Eli Schwartz](https://github.com/EliSchwartz/imagenet-sample-images) (1,000 images) and the so-called [ImageNet-C dataset](https://github.com/hendrycks/robustness) (3,000 images)